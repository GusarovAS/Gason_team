{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pektu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of        Unnamed: 0                                             review  label\n",
       "0               0  I think they really let the quality of the DVD...      0\n",
       "1               1  I'm sorry but this is just awful. I have told ...      0\n",
       "2               2  The Japenese sense of pacing, editing and musi...      0\n",
       "3               3  In the '60's/'70's, David Jason was renowned f...      1\n",
       "4               4  \"Hail The Woman\" is one of the most moving fil...      1\n",
       "...           ...                                                ...    ...\n",
       "39995       39995  When you come across a gem of a movie like thi...      1\n",
       "39996       39996  I don't often go out of my way to write commen...      0\n",
       "39997       39997  This is an extremely silly and little seen fil...      0\n",
       "39998       39998  Just saw the movie, and the scary thing was, t...      1\n",
       "39999       39999  ...though for a film that seems to be trying t...      0\n",
       "\n",
       "[40000 rows x 3 columns]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = 'train.csv'\n",
    "df = pd.read_csv(data)\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I think they really let the quality of the DVD production get away from them. I rented this DVD from 2 movie stores and the second time I finally got it to play on the 3rd DVD player I tried.<br /><br />Anyone else have this issue? It's really hard to give the film an un-biased review after going through such a hassle to play it. For one, I've never seen a Finnish horror film before so I was sort of bummed that the movie was done in English. Also since it's never made clear what is wrong with Sarah, she just came off as retarded and therefore I really just hoped someone would shoot her in the face and make all the horrific happenings go away.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I think they really let the quality of the DVD production get away from them. I rented this DVD from 2 movie stores and the second time I finally got it to play on the 3rd DVD player I tried.  Anyone else have this issue? It's really hard to give the film an un-biased review after going through such a hassle to play it. For one, I've never seen a Finnish horror film before so I was sort of bummed that the movie was done in English. Also since it's never made clear what is wrong with Sarah, she just came off as retarded and therefore I really just hoped someone would shoot her in the face and make all the horrific happenings go away.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][0].replace('<br />', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>excl_number</th>\n",
       "      <th>q_number</th>\n",
       "      <th>neg_number</th>\n",
       "      <th>pos_number</th>\n",
       "      <th>comm_number</th>\n",
       "      <th>double_excl_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I think they really let the quality of the DVD...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003371</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003252</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm sorry but this is just awful. I have told ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003480</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001678</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The Japenese sense of pacing, editing and musi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010511</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>In the '60's/'70's, David Jason was renowned f...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002572</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011702</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>\"Hail The Woman\" is one of the most moving fil...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010711</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>39995</td>\n",
       "      <td>When you come across a gem of a movie like thi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002259</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011423</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>39996</td>\n",
       "      <td>I don't often go out of my way to write commen...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005998</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>39997</td>\n",
       "      <td>This is an extremely silly and little seen fil...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003639</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>39998</td>\n",
       "      <td>Just saw the movie, and the scary thing was, t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009911</td>\n",
       "      <td>0.005896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>39999</td>\n",
       "      <td>...though for a film that seems to be trying t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004088</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013091</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                             review  label  \\\n",
       "0               0  I think they really let the quality of the DVD...      0   \n",
       "1               1  I'm sorry but this is just awful. I have told ...      0   \n",
       "2               2  The Japenese sense of pacing, editing and musi...      0   \n",
       "3               3  In the '60's/'70's, David Jason was renowned f...      1   \n",
       "4               4  \"Hail The Woman\" is one of the most moving fil...      1   \n",
       "...           ...                                                ...    ...   \n",
       "39995       39995  When you come across a gem of a movie like thi...      1   \n",
       "39996       39996  I don't often go out of my way to write commen...      0   \n",
       "39997       39997  This is an extremely silly and little seen fil...      0   \n",
       "39998       39998  Just saw the movie, and the scary thing was, t...      1   \n",
       "39999       39999  ...though for a film that seems to be trying t...      0   \n",
       "\n",
       "       excl_number  q_number  neg_number  pos_number  comm_number  \\\n",
       "0         0.000000  0.003371         0.0         0.0     0.003252   \n",
       "1         0.000000  0.003480         0.0         0.0     0.001678   \n",
       "2         0.000000  0.000000         0.0         0.0     0.010511   \n",
       "3         0.002572  0.000000         0.0         0.0     0.011702   \n",
       "4         0.000000  0.000000         0.0         0.0     0.010711   \n",
       "...            ...       ...         ...         ...          ...   \n",
       "39995     0.002259  0.000000         0.0         0.0     0.011423   \n",
       "39996     0.001977  0.000000         0.0         0.0     0.005998   \n",
       "39997     0.003639  0.000000         0.0         0.0     0.000000   \n",
       "39998     0.009800  0.000000         0.0         0.0     0.009911   \n",
       "39999     0.004088  0.000952         0.0         0.0     0.013091   \n",
       "\n",
       "       double_excl_number  \n",
       "0                0.000000  \n",
       "1                0.000000  \n",
       "2                0.000000  \n",
       "3                0.000000  \n",
       "4                0.000000  \n",
       "...                   ...  \n",
       "39995            0.000000  \n",
       "39996            0.000000  \n",
       "39997            0.000000  \n",
       "39998            0.005896  \n",
       "39999            0.000000  \n",
       "\n",
       "[40000 rows x 9 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pektu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pektu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pektu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "en_stop = list(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = [t for t in tokens if\n",
    "              re.match(r'[^\\W\\d]*$', t) and (len(t) > 2) and (t not in en_stop)]\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return tokens\n",
    "\n",
    "tokens = df['review'].apply(tokenize)\n",
    "#tokens = [simple_preprocess(remove_stopwords(review), deacc=True) for review in df['review']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokens = [[porter_stemmer.stem(word) for word in review] for review in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['think',\n",
       " 'realli',\n",
       " 'let',\n",
       " 'qualiti',\n",
       " 'dvd',\n",
       " 'product',\n",
       " 'get',\n",
       " 'awai',\n",
       " 'rent',\n",
       " 'dvd',\n",
       " 'movi',\n",
       " 'store',\n",
       " 'second',\n",
       " 'time',\n",
       " 'final',\n",
       " 'got',\n",
       " 'plai',\n",
       " 'dvd',\n",
       " 'player',\n",
       " 'tri',\n",
       " 'anyon',\n",
       " 'els',\n",
       " 'issu',\n",
       " 'realli',\n",
       " 'hard',\n",
       " 'give',\n",
       " 'film',\n",
       " 'review',\n",
       " 'go',\n",
       " 'hassl',\n",
       " 'plai',\n",
       " 'on',\n",
       " 'never',\n",
       " 'seen',\n",
       " 'finnish',\n",
       " 'horror',\n",
       " 'film',\n",
       " 'sort',\n",
       " 'bum',\n",
       " 'movi',\n",
       " 'done',\n",
       " 'english',\n",
       " 'also',\n",
       " 'sinc',\n",
       " 'never',\n",
       " 'made',\n",
       " 'clear',\n",
       " 'wrong',\n",
       " 'sarah',\n",
       " 'came',\n",
       " 'retard',\n",
       " 'therefor',\n",
       " 'realli',\n",
       " 'hope',\n",
       " 'someon',\n",
       " 'would',\n",
       " 'shoot',\n",
       " 'face',\n",
       " 'make',\n",
       " 'horrif',\n",
       " 'happen',\n",
       " 'awai']"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(tokens, size=300, window=11, sample=1e-5, min_count=5, iter=10, negative=20, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "#чтобы долго не ждать, можно написать\n",
    "#model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'movie' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-312-7b7eabcc4bf7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'movie'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'film'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36msimilarity\u001b[1;34m(self, w1, w2)\u001b[0m\n\u001b[0;32m    972\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    973\u001b[0m         \"\"\"\n\u001b[1;32m--> 974\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    975\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mn_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mws1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mws2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, entities)\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m             \u001b[1;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 471\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    466\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word 'movie' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "model.wv.similarity('movie', 'film')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df):\n",
    "    df['review'] = df['review'].apply(lambda s: s.replace('<br />', ' '))\n",
    "    df['excl_number'] = tfidf_symb(df['review'], '!')\n",
    "    df['q_number'] = tfidf_symb(df['review'], '?')\n",
    "    #df['dot_number'] = tfidf_symb(df['review'], '.')\n",
    "    df['neg_number'] = tfidf_neg(df['review'])\n",
    "    df['pos_number'] = tfidf_pos(df['review'])\n",
    "    df['comm_number'] = tfidf_symb(df['review'], ',')\n",
    "    df['double_excl_number'] = tfidf_symb(df['review'], '!!')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 300)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode(list_of_tokens):\n",
    "    x = np.array([model.wv[t] for t in list_of_tokens if t in model.wv.vocab])\n",
    "    return np.mean(x, axis=0)\n",
    "\n",
    "fts = np.array([encode(t) for t in tokens])\n",
    "fts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def get_neg(s):\n",
    "    a = s.count('('); b = s.count(')')\n",
    "    if a - b <= 0: return 0\n",
    "    else: return (a - b) / len(s)\n",
    "def get_pos(s):\n",
    "    a = s.count('('); b = s.count(')')\n",
    "    if b - a <= 0: return 0\n",
    "    else: return (b - a) / len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_symb(texts, sym):\n",
    "    idf = np.log((len(texts) + 1) / (np.sum(texts.apply(lambda s: 0 if s.find(sym) == -1 else 1)) + 1)) + 1\n",
    "    res = []\n",
    "    for text in texts:\n",
    "        tf = text.count(sym) / len(text)\n",
    "        res.append(tf * idf)\n",
    "    return res\n",
    "def tfidf_neg(texts):\n",
    "    idf = np.log((len(texts) + 1) / (np.sum(texts.apply(lambda s: 0 if get_neg(s) == 0 else 1)) + 1)) + 1\n",
    "    res = []\n",
    "    for text in texts:\n",
    "        tf = get_neg(text) / len(text)\n",
    "        res.append(tf * idf)\n",
    "    return res\n",
    "def tfidf_pos(texts):\n",
    "    idf = np.log((len(texts) + 1) / (np.sum(texts.apply(lambda s: 0 if get_pos(s) == 0 else 1)) + 1)) + 1\n",
    "    res = []\n",
    "    for text in texts:\n",
    "        tf = get_pos(text) / len(text)\n",
    "        res.append(tf * idf)\n",
    "    return res\n",
    "\n",
    "import re\n",
    "def count_upper(s):\n",
    "    return len(re.findall(r'[A-Z]',s))\n",
    "\n",
    "\n",
    "def tfidf_big(texts):\n",
    "    res = []\n",
    "    for text in texts:\n",
    "        n = count_upper(text)\n",
    "        tf = n / len(text)\n",
    "        idf = np.log((len(texts) + 1) / (np.sum(texts.apply(lambda s: 0 if count_upper(s) >= n else 1)) + 1)) + 1\n",
    "        res.append(tf * idf)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_fts(words, df):\n",
    "    names = ['excl_number', 'double_excl_number', 'q_number', 'neg_number', 'pos_number', 'comm_number']\n",
    "    for name in names:\n",
    "        words = np.hstack([words, np.array(df[name]).reshape(df[name].shape[0], 1)])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "fts = preprocess_fts(fts, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(fts, df.label.values,\n",
    "                                                    test_size=0.2, shuffle=True, random_state = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler(with_mean = False)\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#param_grid = {'C': [0.01, 0.05, 0.1, 0.5, 1, 5, 10]}\n",
    "#estimator = LogisticRegression(solver='lbfgs')\n",
    "#cv = 3\n",
    "#optimizer = GridSearchCV(estimator, param_grid, cv=cv, scoring='f1')\n",
    "#optimizer.fit(X_train_sc, y_train)\n",
    "\n",
    "#clf = DecisionTreeClassifier().fit(X_train_sc, y_train)\n",
    "clf_1 = LogisticRegression(solver='lbfgs', max_iter=1000).fit(X_train_sc, y_train)\n",
    "#clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=7, random_state=0).fit(X_train, y_train)\n",
    "#clf = RandomForestClassifier(n_estimators = 100, max_depth=11, random_state=0).fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model test f1 score: 0.881407\n",
      "Model train f1 score: 0.887587\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = clf_1.predict(X_test_sc)\n",
    "y_pred_train = clf_1.predict(X_train_sc)\n",
    "print('Model test f1 score: {0:0.6f}'. format(f1_score(y_test, y_pred_test)))#0.869346\n",
    "print('Model train f1 score: {0:0.6f}'. format(f1_score(y_train, y_pred_train)))#0.873091"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1774851 , -0.309678  ,  0.3801595 , ..., -0.2632574 ,\n",
       "         0.12917395, -0.17459098],\n",
       "       [ 0.25370404, -0.1012639 ,  0.23353067, ..., -0.32869464,\n",
       "        -0.1847373 ,  0.05554778],\n",
       "       [ 0.32148942, -0.10037386,  0.06013016, ..., -0.2869444 ,\n",
       "         0.01538488, -0.1491847 ],\n",
       "       ...,\n",
       "       [ 0.44015813,  0.07379388, -0.07465646, ..., -0.6674947 ,\n",
       "        -0.18868934,  0.14036416],\n",
       "       [ 0.28908467, -0.40346143, -0.04919094, ..., -0.1637901 ,\n",
       "        -0.03601248, -0.61102384],\n",
       "       [ 0.33260176, -0.32324767,  0.06455156, ..., -0.33922356,\n",
       "        -0.04531411, -0.12442191]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of       Unnamed: 0                                             review  \\\n",
       "0              0  The make -or-break of a love story for me is w...   \n",
       "1              1  \"Bend It Like Beckham\" is a film that got very...   \n",
       "2              2  Pete's Meteor. I seen this referred to as \"aut...   \n",
       "3              3  Funny that I find myself forced to review this...   \n",
       "4              4  Bare Wench is another softcore parody of the B...   \n",
       "...          ...                                                ...   \n",
       "9995        9995  The only reason I wanted to see this was becau...   \n",
       "9996        9996  Three Russian aristocrats soak up the decadenc...   \n",
       "9997        9997  Greetings again from the darkness. Remember al...   \n",
       "9998        9998  This film is a tour de force from Julie Taymor...   \n",
       "9999        9999  I'll be honest,I finally checked this movie no...   \n",
       "\n",
       "      excl_number  q_number  neg_number  pos_number  comm_number  \\\n",
       "0        0.001547  0.000000         0.0         0.0     0.005447   \n",
       "1        0.002798  0.000000         0.0         0.0     0.007036   \n",
       "2        0.000000  0.000867         0.0         0.0     0.007723   \n",
       "3        0.000000  0.000474         0.0         0.0     0.018935   \n",
       "4        0.009982  0.000000         0.0         0.0     0.010039   \n",
       "...           ...       ...         ...         ...          ...   \n",
       "9995     0.000000  0.000000         0.0         0.0     0.011496   \n",
       "9996     0.000000  0.000000         0.0         0.0     0.019489   \n",
       "9997     0.003582  0.001871         0.0         0.0     0.008106   \n",
       "9998     0.000000  0.000000         0.0         0.0     0.004883   \n",
       "9999     0.003319  0.001155         0.0         0.0     0.009456   \n",
       "\n",
       "      double_excl_number  \n",
       "0               0.000000  \n",
       "1               0.000000  \n",
       "2               0.000000  \n",
       "3               0.000000  \n",
       "4               0.000000  \n",
       "...                  ...  \n",
       "9995            0.000000  \n",
       "9996            0.000000  \n",
       "9997            0.003242  \n",
       "9998            0.000000  \n",
       "9999            0.002003  \n",
       "\n",
       "[10000 rows x 8 columns]>"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datat = 'test.csv'\n",
    "df_test = pd.read_csv(datat)\n",
    "dft.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review</th>\n",
       "      <th>excl_number</th>\n",
       "      <th>q_number</th>\n",
       "      <th>neg_number</th>\n",
       "      <th>pos_number</th>\n",
       "      <th>comm_number</th>\n",
       "      <th>double_excl_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The make -or-break of a love story for me is w...</td>\n",
       "      <td>0.001547</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005447</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\"Bend It Like Beckham\" is a film that got very...</td>\n",
       "      <td>0.002798</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007036</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Pete's Meteor. I seen this referred to as \"aut...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007723</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Funny that I find myself forced to review this...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018935</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Bare Wench is another softcore parody of the B...</td>\n",
       "      <td>0.009982</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010039</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9995</td>\n",
       "      <td>The only reason I wanted to see this was becau...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011496</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9996</td>\n",
       "      <td>Three Russian aristocrats soak up the decadenc...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019489</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9997</td>\n",
       "      <td>Greetings again from the darkness. Remember al...</td>\n",
       "      <td>0.003582</td>\n",
       "      <td>0.001871</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008106</td>\n",
       "      <td>0.003242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9998</td>\n",
       "      <td>This film is a tour de force from Julie Taymor...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>9999</td>\n",
       "      <td>I'll be honest,I finally checked this movie no...</td>\n",
       "      <td>0.003319</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009456</td>\n",
       "      <td>0.002003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                             review  \\\n",
       "0              0  The make -or-break of a love story for me is w...   \n",
       "1              1  \"Bend It Like Beckham\" is a film that got very...   \n",
       "2              2  Pete's Meteor. I seen this referred to as \"aut...   \n",
       "3              3  Funny that I find myself forced to review this...   \n",
       "4              4  Bare Wench is another softcore parody of the B...   \n",
       "...          ...                                                ...   \n",
       "9995        9995  The only reason I wanted to see this was becau...   \n",
       "9996        9996  Three Russian aristocrats soak up the decadenc...   \n",
       "9997        9997  Greetings again from the darkness. Remember al...   \n",
       "9998        9998  This film is a tour de force from Julie Taymor...   \n",
       "9999        9999  I'll be honest,I finally checked this movie no...   \n",
       "\n",
       "      excl_number  q_number  neg_number  pos_number  comm_number  \\\n",
       "0        0.001547  0.000000         0.0         0.0     0.005447   \n",
       "1        0.002798  0.000000         0.0         0.0     0.007036   \n",
       "2        0.000000  0.000867         0.0         0.0     0.007723   \n",
       "3        0.000000  0.000474         0.0         0.0     0.018935   \n",
       "4        0.009982  0.000000         0.0         0.0     0.010039   \n",
       "...           ...       ...         ...         ...          ...   \n",
       "9995     0.000000  0.000000         0.0         0.0     0.011496   \n",
       "9996     0.000000  0.000000         0.0         0.0     0.019489   \n",
       "9997     0.003582  0.001871         0.0         0.0     0.008106   \n",
       "9998     0.000000  0.000000         0.0         0.0     0.004883   \n",
       "9999     0.003319  0.001155         0.0         0.0     0.009456   \n",
       "\n",
       "      double_excl_number  \n",
       "0               0.000000  \n",
       "1               0.000000  \n",
       "2               0.000000  \n",
       "3               0.000000  \n",
       "4               0.000000  \n",
       "...                  ...  \n",
       "9995            0.000000  \n",
       "9996            0.000000  \n",
       "9997            0.003242  \n",
       "9998            0.000000  \n",
       "9999            0.002003  \n",
       "\n",
       "[10000 rows x 8 columns]"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_df(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_test = df_test['review'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 300)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fts_test = np.array([encode(t) for t in tokens_test])\n",
    "fts_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 306)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fts_test = preprocess_fts(fts_test, df_test)\n",
    "fts_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = scaler.fit_transform(fts)\n",
    "fts_test_sc = scaler.transform(fts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_1.fit(X_all, df['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89     20067\n",
      "           1       0.89      0.89      0.89     19933\n",
      "\n",
      "    accuracy                           0.89     40000\n",
      "   macro avg       0.89      0.89      0.89     40000\n",
      "weighted avg       0.89      0.89      0.89     40000\n",
      "\n",
      "Model f1 score: 0.890435\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df['label'].values, clf_1.predict(X_all)))\n",
    "print('Model f1 score: {0:0.6f}'. format(f1_score(df['label'].values, clf_1.predict(X_all))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = clf_1.predict(fts_test_sc)\n",
    "ans = pd.DataFrame(np.array([dft['Unnamed: 0'], res]).T, columns=[['Id', 'Predicted']])\n",
    "ans.to_csv('Gason_task_4.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
